import os
import numpy as np
import tensorflow as tf

def deep_model_diagnosis(model_path):
    """
    æ·±åº¦è¨ºæ–·æ¨¡å‹å…§éƒ¨
    """
    print("\n" + "="*80)
    print("ğŸ”¬ DEEP MODEL INTERNAL DIAGNOSIS")
    print("="*80)
    
    model = tf.keras.models.load_model(model_path, compile=False)
    
    # =========================================================================
    # 1. æ¨¡å‹æ¶æ§‹
    # =========================================================================
    print("\n[1/5] MODEL ARCHITECTURE")
    print("-"*80)
    model.summary()
    
    # =========================================================================
    # 2. æª¢æŸ¥æ‰€æœ‰å±¤çš„æ¬Šé‡
    # =========================================================================
    print("\n[2/5] LAYER WEIGHTS ANALYSIS")
    print("-"*80)
    
    critical_issues = []
    
    for i, layer in enumerate(model.layers):
        weights = layer.get_weights()
        if not weights:
            continue
        
        print(f"\nLayer {i}: {layer.name} ({layer.__class__.__name__})")
        
        # æª¢æŸ¥ kernel (æ¬Šé‡)
        if len(weights) > 0:
            kernel = weights[0]
            print(f"  Kernel shape: {kernel.shape}")
            print(f"    Mean:     {kernel.mean():10.6f}")
            print(f"    Std:      {kernel.std():10.6f}")
            print(f"    Min:      {kernel.min():10.6f}")
            print(f"    Max:      {kernel.max():10.6f}")
            print(f"    Abs mean: {np.abs(kernel).mean():10.6f}")
            
            # æª¢æŸ¥ç•°å¸¸
            if kernel.std() < 1e-7:
                print(f"    âŒ CRITICAL: Kernel has ZERO variance!")
                critical_issues.append(f"Layer {i} ({layer.name}): Zero variance kernel")
            
            if np.abs(kernel).mean() < 1e-7:
                print(f"    âŒ CRITICAL: Kernel weights are all near ZERO!")
                critical_issues.append(f"Layer {i} ({layer.name}): Near-zero weights")
        
        # æª¢æŸ¥ bias
        if len(weights) > 1:
            bias = weights[1]
            print(f"  Bias shape: {bias.shape}")
            print(f"    Mean: {bias.mean():10.6f}")
            print(f"    Std:  {bias.std():10.6f}")
            print(f"    Min:  {bias.min():10.6f}")
            print(f"    Max:  {bias.max():10.6f}")
    
    # =========================================================================
    # 3. æª¢æŸ¥æ¿€æ´»å‡½æ•¸
    # =========================================================================
    print("\n[3/5] ACTIVATION FUNCTIONS")
    print("-"*80)
    
    for i, layer in enumerate(model.layers):
        if hasattr(layer, 'activation'):
            activation_name = layer.activation.__name__ if hasattr(layer.activation, '__name__') else str(layer.activation)
            print(f"Layer {i:2d} ({layer.name:20s}): {activation_name}")
            
            # æª¢æŸ¥ Dense å±¤æ˜¯å¦ç¼ºå°‘æ¿€æ´»
            if isinstance(layer, tf.keras.layers.Dense):
                if 'prediction' not in layer.name and activation_name == 'linear':
                    print(f"    âš ï¸  WARNING: Dense layer without activation!")
                    critical_issues.append(f"Layer {i} ({layer.name}): Missing activation")
    
    # =========================================================================
    # 4. æ¸¬è©¦å‰å‘å‚³æ’­ - ä½¿ç”¨æ¨¡å‹å¯¦éš›æ¨è«–
    # =========================================================================
    print("\n[4/5] FORWARD PROPAGATION TEST")
    print("-"*80)
    
    # å‰µå»ºä¸‰å€‹éå¸¸ä¸åŒçš„æ¸¬è©¦è¼¸å…¥
    test_cases = {
        'All zeros': np.zeros((1, 224, 224, 3), dtype=np.uint8),
        'All 255s': np.ones((1, 224, 224, 3), dtype=np.uint8) * 255,
        'Random': np.random.randint(0, 256, (1, 224, 224, 3), dtype=np.uint8),
    }
    
    print("\nTesting model with different inputs:")
    outputs = {}
    for name, test_input in test_cases.items():
        out = model(test_input, training=False).numpy()[0]
        outputs[name] = out
        print(f"  {name:12s}: {out}")
    
    # æª¢æŸ¥è¼¸å‡ºæ˜¯å¦ç›¸åŒ
    output_values = list(outputs.values())
    all_same = all(np.allclose(output_values[0], out) for out in output_values[1:])
    
    if all_same:
        print(f"\n  âŒ CRITICAL: All outputs are IDENTICAL!")
        critical_issues.append("Model produces constant output for all inputs")
    else:
        max_diff = max(np.abs(output_values[i] - output_values[j]).max() 
                      for i in range(len(output_values)) 
                      for j in range(i+1, len(output_values)))
        print(f"\n  Max output difference: {max_diff:.8f}")
        
        if max_diff < 1e-6:
            critical_issues.append("Model outputs are nearly identical")
    
    # =========================================================================
    # 5. æœ€çµ‚è¼¸å‡ºå±¤ç‰¹åˆ¥æª¢æŸ¥
    # =========================================================================
    print("\n[5/5] FINAL OUTPUT LAYER ANALYSIS")
    print("-"*80)
    
    final_layer = model.layers[-1]
    print(f"\nFinal layer: {final_layer.name}")
    print(f"  Type: {final_layer.__class__.__name__}")
    
    if hasattr(final_layer, 'activation'):
        print(f"  Activation: {final_layer.activation.__name__}")
    
    # æª¢æŸ¥æœ€å¾Œä¸€å±¤çš„æ¬Šé‡
    final_weights = final_layer.get_weights()
    if final_weights:
        final_kernel = final_weights[0]
        final_bias = final_weights[1] if len(final_weights) > 1 else None
        
        print(f"\n  Kernel:")
        print(f"    Shape: {final_kernel.shape}")
        print(f"    Mean:  {final_kernel.mean():10.6f}")
        print(f"    Std:   {final_kernel.std():10.6f}")
        
        if final_bias is not None:
            print(f"\n  Bias:")
            print(f"    Values: {final_bias}")
            
            # è¨ˆç®— sigmoid(bias)
            sigmoid_bias = 1 / (1 + np.exp(-final_bias))
            
            print(f"\n  If model were just sigmoid(bias):")
            print(f"    sigmoid(bias) = {sigmoid_bias}")
            
            # èˆ‡å¯¦éš›è¼¸å‡ºæ¯”è¼ƒ
            expected_output = outputs['All zeros']  # ä½¿ç”¨å…¨é›¶è¼¸å…¥çš„è¼¸å‡º
            print(f"    Actual output = {expected_output}")
            print(f"    Difference    = {np.abs(sigmoid_bias - expected_output)}")
            
            diff = np.abs(sigmoid_bias - expected_output).max()
            if diff < 0.01:
                print(f"\n  âŒ SMOKING GUN: Output â‰ˆ sigmoid(bias)!")
                print(f"     Difference is only {diff:.6f}")
                print(f"     Model is IGNORING all inputs!")
                critical_issues.append("Model output = sigmoid(bias) â†’ completely ignoring inputs")
            else:
                print(f"\n  Output is NOT just sigmoid(bias) (diff={diff:.6f})")
    
    # =========================================================================
    # ç¸½çµ
    # =========================================================================
    print("\n" + "="*80)
    print("CRITICAL ISSUES SUMMARY")
    print("="*80)
    
    if critical_issues:
        print(f"\nFound {len(critical_issues)} critical issues:\n")
        for i, issue in enumerate(critical_issues, 1):
            print(f"{i}. {issue}")
    else:
        print("\nNo critical structural issues found.")
        print("(But model still produces constant outputs)")
    
    print("\n" + "="*80)
    
    return critical_issues


# =========================
# Main
# =========================
if __name__ == '__main__':
    checkpoint_dir = 'checkpoints/scratch_aug/'
    
    print("\n" + "ğŸ”¬"*40)
    print("DETAILED MODEL DIAGNOSIS")
    print("ğŸ”¬"*40)
    
    # æª¢æŸ¥ Epoch 450
    model_path = os.path.join(checkpoint_dir, 'Epoch_450_model.h5')
    issues = deep_model_diagnosis(model_path)
    
    # è©³ç´°å»ºè­°
    print("\n" + "ğŸ’¡"*40)
    print("ROOT CAUSE ANALYSIS & RECOMMENDATIONS")
    print("ğŸ’¡"*40)
    
    print("\n" + "="*80)
    print("ROOT CAUSE:")
    print("="*80)
    
    has_missing_activation = any("Missing activation" in issue for issue in issues)
    outputs_constant = any("constant output" in issue.lower() for issue in issues)
    ignoring_inputs = any("ignoring inputs" in issue.lower() for issue in issues)
    
    if has_missing_activation:
        print("\nâŒ CRITICAL PROBLEM: Dense layers lack activation functions")
        print("\nYour training code:")
        print("  model.add(kl.Dense(1024))  â† NO activation!")
        print("  model.add(kl.Dense(256))   â† NO activation!")
        
        print("\nThis causes the model to collapse into a single linear transformation:")
        print("  output = sigmoid(W3 @ W2 @ W1 @ input + combined_bias)")
        print("         = sigmoid(W_combined @ input + bias_combined)")
        
        print("\nDuring training, the model learned that the best 'linear' strategy")
        print("is to just output constant values (ignore input, only use bias).")
    
    if outputs_constant or ignoring_inputs:
        print("\nâŒ CONFIRMED: Model produces constant outputs")
        print("\nThe model has learned to completely ignore the input and")
        print("only rely on the final bias term to produce outputs.")
    
    print("\n" + "="*80)
    print("SOLUTION:")
    print("="*80)
    
    print("\nâœ… You MUST fix the training script and retrain:")
    
    print("\n1. Fix the model architecture (add activations):")
    print("```python")
    print("model.add(kl.Flatten())")
    print("model.add(kl.Dense(1024, activation='relu'))  # â† ADD activation='relu'")
    print("model.add(kl.Dropout(0.5))")
    print("model.add(kl.Dense(256, activation='relu'))   # â† ADD activation='relu'")
    print("model.add(kl.Dropout(0.3))")
    print("model.add(kl.Dense(4, activation='sigmoid'))")
    print("```")
    
    print("\n2. Fix the training function bugs:")
    print("```python")
    print("@tf.function")
    print("def train_step(x, y):  # â† Remove extra parameters")
    print("    with tf.GradientTape() as tape:")
    print("        logits = model(x, training=True)  # â† Add training=True")
    print("        loss_value = loss_fn(y, logits)")
    print("    grads = tape.gradient(loss_value, model.trainable_weights)")
    print("    optimizer.apply_gradients(zip(grads, model.trainable_weights))")
    print("    return loss_value")
    print("```")
    
    print("\n3. Monitor training closely:")
    print("   - Loss should DECREASE steadily")
    print("   - F1 score should INCREASE")
    print("   - Test model every 50 epochs with random inputs")
    print("   - Outputs should be DIFFERENT for different inputs")
    
    print("\n4. After retraining, verify the model:")
    print("```python")
    print("# Quick test")
    print("test1 = np.zeros((1,224,224,3), dtype=np.uint8)")
    print("test2 = np.ones((1,224,224,3), dtype=np.uint8)*255")
    print("out1 = model(test1).numpy()[0]")
    print("out2 = model(test2).numpy()[0]")
    print("print(f'Difference: {np.abs(out1-out2).max():.6f}')")
    print("# Should be > 0.01 if model works!")
    print("```")
    
    print("\n" + "="*80)
    print("\nâš ï¸  Current model is UNFIXABLE - you cannot just add activations")
    print("    to the saved model. You MUST retrain from scratch.")
    print("\n" + "="*80 + "\n")